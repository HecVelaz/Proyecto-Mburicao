{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "393f955d-ac85-4aad-b14b-9920ad84ae85",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2925056311.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    num_features = X.shape[1] Definir un pipeline que primero selecciona las K mejores features usando f_regression\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "data = pd.read_csv('/home/nivelrios/documentos/Mburicao Project/src/data/data_extraction.csv')\n",
    "# Copia del DataFrame original\n",
    "df = data.copy()\n",
    "\n",
    "# Convertir 'fecha' a datetime y descartar esa columna para el modelo\n",
    "df['fecha'] = pd.to_datetime(df['fecha'], errors='coerce')\n",
    "X = df.drop(columns=[\"fecha\", \"global_peak\"])\n",
    "y = df[\"global_peak\"]\n",
    "\n",
    "num_features = X.shape[1] Definir un pipeline que primero selecciona las K mejores features usando f_regression\n",
    "# y luego entrena un modelo de regresión lineal.\n",
    "pipeline = Pipeline([\n",
    "    ('select', SelectKBest(score_func=f_regression)),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Definir la rejilla de parámetros para la búsqueda:\n",
    "# 'select__k' indica el número de features a seleccionar.\n",
    "# 'model__fit_intercept' define si se ajusta el intercepto del modelo o no.\n",
    "param_grid = {\n",
    "    'select__k': list(range(1, num_features + 1)) + ['all'],\n",
    "    'model__fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Configurar validación cruzada con KFold (5 pliegues, con shuffle)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Configurar GridSearchCV para buscar la mejor combinación de hiperparámetros\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=kf, scoring='r2', n_jobs=-1, refit=True)\n",
    "grid_search.fit(X, y)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n",
    "\n",
    "# --- Obtener las características seleccionadas ---\n",
    "# Se accede al paso 'select' del pipeline y se utiliza get_support() para obtener la máscara booleana.\n",
    "mask = best_model.named_steps['select'].get_support()\n",
    "selected_features = X.columns[mask]\n",
    "print(\"Features seleccionadas:\", list(selected_features))\n",
    "\n",
    "# Utilizar cross_val_predict para obtener predicciones fuera de la muestra usando el modelo óptimo\n",
    "y_pred = cross_val_predict(best_model, X, y, cv=kf)\n",
    "\n",
    "# Calcular las métricas de rendimiento\n",
    "r2 = r2_score(y, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "mape = mean_absolute_percentage_error(y, y_pred)*100\n",
    "\n",
    "print(f\"R²: {r2:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"MAPE: {mape:.3f}\")\n",
    "\n",
    "# Graficar valores reales vs. predichos\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y, y_pred, color='blue', alpha=0.7, label='Predicciones')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2, label='Ideal')\n",
    "plt.xlabel('Valor Real')\n",
    "plt.ylabel('Valor Predicho')\n",
    "plt.title('Valor Real vs. Valor Predicho')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5fdd0f-396a-4c00-87c3-f74cb05f6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"global_peak\"])\n",
    "y = df[\"global_peak\"].values.reshape(-1, 1)  \n",
    "if len(y.shape) == 1:\n",
    "    y = y.reshape(-1, 1)\n",
    "scaler_X = MinMaxScaler()  \n",
    "scaler_y = MinMaxScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "y_normalized = scaler_y.fit_transform(y)\n",
    "model = LinearRegression()\n",
    "rfecv = RFECV(estimator=model, step=1, cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "              scoring='r2')\n",
    "\n",
    "rfecv.fit(X_normalized, y_normalized)\n",
    "selected_features = X.columns[rfecv.support_]     \n",
    "print(\"Número óptimo de features:\", rfecv.n_features_)\n",
    "selected_features = X.columns[rfecv.support_]\n",
    "print(\"Features seleccionadas:\", list(selected_features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
